# -*- coding: utf-8 -*-
"""pattern (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XFRJQWys6NdXr7Yd4KHREE7S3_P8Ys1V
"""
import cloudpickle
from file import CounterEncoder
from file import KNN_PP
from file import MeansAndMods
from file import OHE
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from scipy.stats import kendalltau

from sklearn.feature_selection import SelectKBest, chi2
import pandas as pd
from scipy.stats import kendalltau
from sklearn.linear_model import LogisticRegression
import pickle

data = pd.read_csv('ApartmentRentPrediction_Milestone2.csv')
data.head(10)

"""put the prediction column at the end"""

predicted_col = data['RentCategory']
data.drop(columns=['RentCategory'], inplace=True)
data['RentCategory'] = predicted_col

# Assuming you have a DataFrame named 'df' with your data

# Splitting the DataFrame into features (X) and target variable (y)
X = data.drop(columns=['RentCategory'])
Y = data['RentCategory']

# Splitting the data into training and test sets (80% training, 20% test)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Concatenating X_train and y_train
train_data = pd.concat([X_train, Y_train], axis=1)

# Concatenating X_test and y_test
test_data = pd.concat([X_test, Y_test], axis=1)

for column in train_data:
    print(train_data[column].value_counts() , '\n')

"""number of nulls in train"""
for column in train_data.columns:
  print(column, '\t', train_data[column].isna().sum())

"""number of nulls in test"""
for column in test_data.columns:
  print(column, '\t', test_data[column].isna().sum())

"""replace each row in amenities with the **count** of the amenities"""
ce = CounterEncoder()
train_data['amenities'] = ce.transform(train_data['amenities'])
test_data['amenities'] = ce.transform(test_data['amenities'])

"""remove noise"""
print(train_data['category'].value_counts())
train_data = train_data.drop(train_data[(train_data['category'] == 'housing/rent/short_term') | (train_data['category'] == 'housing/rent/home')].index)
print(train_data['category'].value_counts())


mam = MeansAndMods()
mam.fit(train_data)

#to be commented
# categoryModeTrain = train_data['category'].mode()[0]

# print(test_data['category'].value_counts())

# for index, value in test_data['category'].items():
#     if value != categoryModeTrain:
#         test_data.at[index, 'category'] = categoryModeTrain

# print(test_data['category'].value_counts())

"""**replace nulls of longitude and latitude first**"""

train_data.dropna(subset=['longitude', 'latitude'], inplace=True)

test_data['longitude'] = mam.transform(test_data['longitude'])
test_data['latitude'] = mam.transform(test_data['latitude'])

# Note: The following line is Required as we want these 2 cols without any nulls


city_KP = KNN_PP()
train_data['cityname'] = city_KP.fit_transform(train_data.loc[:, ['latitude', 'longitude']], train_data['cityname'])
test_data['cityname'] = city_KP.transform(test_data.loc[:, ['latitude', 'longitude']], test_data['cityname'])

state_KP = KNN_PP()
train_data['state'] = state_KP.fit_transform(train_data.loc[:, ['latitude', 'longitude']], train_data['state'])
test_data['state'] = state_KP.transform(test_data.loc[:, ['latitude', 'longitude']], test_data['state'])

train_data['bathrooms'] = mam.transform(train_data['bathrooms'])
train_data['bedrooms']  = mam.transform(train_data['bedrooms'])

train_data['bathrooms'] = train_data['bathrooms'].astype(int)
train_data['bedrooms'] = train_data['bedrooms'].astype(int)

test_data['bathrooms'] = mam.transform(test_data['bathrooms'])
test_data['bedrooms']  = mam.transform(test_data['bedrooms'])

test_data['bathrooms'] = test_data['bathrooms'].astype(int)
test_data['bedrooms'] = test_data['bedrooms'].astype(int)


print(train_data['pets_allowed'].value_counts())

pets_ohe = OHE(removeCol='pets_allowed')
pets_ohe.fit_transform(train_data)
pets_ohe.transform(test_data)

print(train_data['cats_allowed'].value_counts(), train_data['dogs_allowed'].value_counts())

"""number of nulls"""
for column in train_data.columns:
  print(column, '\t',train_data[column].isna().sum())

"""number of nulls"""
for column in test_data.columns:
  print(column, '\t',test_data[column].isna().sum())

train_data['address'].fillna('Unknown', inplace=True)
mam.fit(train_data)
test_data['address'] = mam.transform(test_data['address'])

for col in train_data.columns:
    train_data[col] = mam.transform(train_data[col])
for col in test_data.columns:
    test_data[col] = mam.transform(test_data[col])

"""number of nulls"""
for column in train_data.columns:
  print(column, '\t',train_data[column].isna().sum())

photo_ohe = OHE('has_photo')
photo_ohe.fit_transform(train_data, 'photo')
train_data.drop(columns=['no_photo'], inplace=True)
photo_ohe.transform(test_data)
test_data.drop(columns=['no_photo'], inplace=True)

def count_outliers(df):
    for column_name in df.columns:
        Q1 = df[column_name].quantile(0.25)
        Q3 = df[column_name].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]
        print(f"Number of outliers in '{column_name}': {outliers.shape[0]}")

outliercolums = ['bathrooms', 'bedrooms', 'square_feet']
print(count_outliers(train_data[outliercolums]))
# pd.set_option('display.max_rows', None)
print(train_data['bedrooms'].value_counts())

sns.boxplot(train_data['square_feet'])

sns.boxplot(train_data['bedrooms'])

sns.boxplot(train_data['bathrooms'])

def replace_outliers(df, columns):
    for column_name in columns:
        # Calculate quartiles
        Q1 = df[column_name].quantile(0.25)
        Q3 = df[column_name].quantile(0.75)
        # Calculate IQR
        IQR = Q3 - Q1
        # Define outlier boundaries
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        # Replace outliers below the lower bound with Q1 - 1.5 * IQR
        df.loc[df[column_name] < lower_bound, column_name] = lower_bound
        # Replace outliers above the upper bound with Q3 + 1.5 * IQR
        df.loc[df[column_name] > upper_bound, column_name] = upper_bound
    return df

# train_data = replace_outliers(train_data, outliercolums)
# print(count_outliers(train_data[outliercolums]))
#print(train_data['square_feet'].max())

train_data.info()

# def Feature_Encoder(X, cols):
#     for c in cols:
#         lbl = LabelEncoder()
#         lbl.fit(list(X[c].values))

#         X[c] = lbl.transform(list(X[c].values))
#     return X

#remove noise
train_data = train_data.drop(train_data[(train_data['price_type'] == 'Weekly') | (train_data['price_type'] == 'Monthly|Weekly')].index)
mam.fit(train_data)

train_data.info()

cols=('category','title','body','cityname', 'state', 'source', 'address','fee','currency','price_type')

lbls = []
for c in cols:
    lbl = LabelEncoder()
    train_data[c] = lbl.fit_transform(list(train_data[c].values))
    lbls.append(lbl)
    encoded_labels = []
    for idx, val in test_data[c].items():
      if val not in lbl.classes_:
        test_data.at[idx,c] = mam.special[c]
    test_data[c] = lbl.transform(test_data[c])

train_data['RentCategory'] = train_data['RentCategory'].replace({'Medium-Priced Rent': 1, 'Low Rent': 0, 'High Rent': 2})
test_data['RentCategory'] = test_data['RentCategory'].replace({'Medium-Priced Rent': 1, 'Low Rent': 0, 'High Rent': 2})

train_data.info()

train_data.head()

from sklearn.preprocessing import MinMaxScaler

RentCategoryData = train_data['RentCategory']
train_data.drop(columns=['RentCategory'], inplace = True)

column_names = train_data.columns

# Scale the data
scaler = MinMaxScaler(feature_range=(0, 1))
print("Train Info")
train_data.info()
train_data_scaled = scaler.fit_transform(train_data)

# Create a new DataFrame with scaled data
train_data_scaled_df = pd.DataFrame(train_data_scaled, columns=column_names)

train_data = train_data_scaled_df
train_data['RentCategory'] = RentCategoryData.values

print(train_data.head())

print(test_data.head())

RentCategoryDataTest = test_data['RentCategory']
test_data.drop(columns=['RentCategory'], inplace=True)
print("columns of test are",test_data.columns)
test_data_scaled = scaler.transform(test_data)

test_data_scaled_df = pd.DataFrame(test_data_scaled, columns=column_names)
test_data= test_data_scaled_df
test_data['RentCategory'] = RentCategoryDataTest.values

print(test_data.head())

train_data.info()

train_data.info()

# train_data.dropna(axis=1, inplace=True)

#train_data.head(7)
train_data.info()

# from sklearn.feature_selection import SelectKBest, chi2

# # Encode categorical variables if not already done. Assuming it's done in your preprocessing.
# # Ensure there are no negative values as chi2 cannot handle them. Chi2 is only for non-negative features and class labels.
# # Since you've handled missing values and encoding, you can directly apply Chi-square.

# # Applying SelectKBest class to extract top 'k' best features
# # Here, 'k' can be any number or you can use 'k='all' to select all features
# k = 10  # Example: select the top 10 features
# bestfeatures = SelectKBest(score_func=chi2, k=k)
# fit = bestfeatures.fit(train_data.drop('RentCategory', axis=1), train_data['RentCategory'])

# # Get the scores for each feature
# dfscores = pd.DataFrame(fit.scores_)
# dfcolumns = pd.DataFrame(train_data.drop('RentCategory', axis=1).columns)

# # Concatenate two dataframes for better visualization
# featureScores = pd.concat([dfcolumns, dfscores], axis=1)
# featureScores.columns = ['Feature', 'Score']  # naming the dataframe columns
# print(featureScores.nlargest(k, 'Score'))  # print k best features

# # You can now decide to keep only the top k features:
# train_data_selected_features = train_data[featureScores.nlargest(k, 'Score')['Feature'].tolist() + ['RentCategory']]

# c = train_data.corr()
# top_features = c.index[abs(c['RentCategory'])>=0.0]
# plt.subplots(figsize=(20, 15))
# top_corr = train_data[top_features].corr()
# sns.heatmap(top_corr, annot=True)
# plt.show()
# print(top_features)
# top_features = top_features.delete(-1)
# print(top_features)

categorical_columns = ['category', 'title', 'body', 'currency', 'fee', 'yes_photo', 'thumbnail_photo', 'cats_allowed', 'dogs_allowed', 'price_type', 'address', 'cityname', 'state', 'source']
numerical_columns = ['id', 'bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'time','amenities']

categorical_data = train_data[categorical_columns]
numerical_data = train_data[numerical_columns]

"""## feature Slectoin

### mutual information
"""

def select_best_features_mutual_info(X, Y, num_features):
    # Compute mutual information scores
    mi_scores = mutual_info_classif(X, Y)

    # Create a DataFrame to store feature names and their corresponding mutual information scores
    mi_df = pd.DataFrame({'Feature': X.columns, 'MI_Score': mi_scores})

    # Sort features based on mutual information scores (descending order)
    mi_df_sorted = mi_df.sort_values(by='MI_Score', ascending=False)

    # Select top features
    selected_features = mi_df_sorted.iloc[:num_features]['Feature'].tolist()

    # Create DataFrame with selected features
    X_top = X[selected_features]

    return X_top.columns.tolist()

"""### chi-squared"""

def select_best_features_chi2(X, Y, num_features):
    # SelectKBest with chi-squared as the scoring function
    selector = SelectKBest(score_func=chi2, k=num_features)
    X_new = selector.fit_transform(X, Y)

    # Get chi-squared scores and corresponding feature names
    chi2_scores = selector.scores_
    feature_names = X.columns

    # Create a DataFrame to store feature names and their corresponding chi-squared scores
    chi2_df = pd.DataFrame({'Feature': feature_names, 'Chi2_Score': chi2_scores})

    # Sort features based on chi-squared scores (descending order)
    chi2_df_sorted = chi2_df.sort_values(by='Chi2_Score', ascending=False)

    # Select top features
    selected_features = chi2_df_sorted.iloc[:num_features]['Feature'].tolist()

    # Create DataFrame with selected features
    X_top = X[selected_features]

    return X_top.columns.tolist()

"""### anova"""

def select_best_features_anova(X, Y, num_features):

    # Perform ANOVA for feature selection
    best_features = SelectKBest(score_func=f_classif, k=num_features)
    fit = best_features.fit(X, Y)

    # Get ANOVA F-values and corresponding feature names
    f_scores = pd.DataFrame(fit.scores_)
    feature_names = pd.DataFrame(X.columns)

    # Combine feature names and their ANOVA F-values
    feature_scores = pd.concat([feature_names, f_scores], axis=1)
    feature_scores.columns = ['Feature', 'Score']  # Naming the DataFrame columns

    # Sort features based on ANOVA F-values (descending order)
    feature_scores_sorted = feature_scores.sort_values(by='Score', ascending=False)

    # Select top features
    selected_features = feature_scores_sorted.iloc[:num_features]['Feature'].tolist()

    # Create DataFrame with selected features
    X_top = X[selected_features]

    return X_top.columns.tolist()

"""###  kendall"""

def select_best_features_kendall_tau(X, Y, num_features):
    # Compute Kendall's tau between each feature and the target
    scores, pvalues = [], []
    for column in X.columns:
        score, pvalue = kendalltau(X[column], Y)
        scores.append(score if pd.notnull(score) else 0)  # handle NaN scores
        pvalues.append(pvalue)

    # Create DataFrame to view scores and p-values
    dfscores = pd.DataFrame(scores, index=X.columns, columns=['Score'])
    dfpvalues = pd.DataFrame(pvalues, index=X.columns, columns=['PValue'])

    # Combine scores and p-values into a single DataFrame
    feature_scores = pd.concat([dfscores, dfpvalues], axis=1)
    feature_scores.reset_index(inplace=True)
    feature_scores.columns = ['Feature', 'Score', 'PValue']  # Rename the DataFrame columns

    # Select the top k features based on the score
    selected_features = feature_scores.nlargest(num_features, 'Score')['Feature'].tolist()

    # Create DataFrame with selected features
    X_top = X[selected_features]

    return X_top.columns.tolist()

categorical_features = select_best_features_chi2(categorical_data, train_data['RentCategory'], 5)
numerical_features = select_best_features_anova(numerical_data, train_data['RentCategory'], 5)

X_top_features = categorical_features + numerical_features
X_top_features

X_train = train_data[X_top_features]
Y_train = train_data['RentCategory']

X_test = test_data[X_top_features]
Y_test = test_data['RentCategory']

"""## models"""

from sklearn import svm
from sklearn.metrics import accuracy_score

svm_classifier = svm.SVC(kernel='linear', C=1)

svm_classifier.fit(X_train, Y_train)

# Make predictions on the training set
Y_train_predicted = svm_classifier.predict(X_train)

# Compute training accuracy
train_accuracy = accuracy_score(Y_train, Y_train_predicted)
print("Train accuracy:", train_accuracy)


# Make predictions on the test set
Y_test_predicted = svm_classifier.predict(X_test)

# Compute test accuracy
test_accuracy = accuracy_score(Y_test, Y_test_predicted)
print("Test accuracy:", test_accuracy)

import xgboost as xgb
from sklearn.metrics import accuracy_score

# Initialize XGBoost classifier
xgb_classifier = xgb.XGBClassifier()

# Train XGBoost classifier
xgb_classifier.fit(X_train, Y_train)

# Make predictions on the training set
Y_train_predicted = xgb_classifier.predict(X_train)

# Compute training accuracy
train_accuracy = accuracy_score(Y_train, Y_train_predicted)
print("Training accuracy:", train_accuracy)

# Make predictions on the test set
Y_test_predicted = xgb_classifier.predict(X_test)

# Compute test accuracy
test_accuracy = accuracy_score(Y_test, Y_test_predicted)
print("Test accuracy:", test_accuracy)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=10, random_state=42)

# Fit on training data
rf_classifier.fit(X_train, Y_train)

# Make predictions on the training set
Y_train_predicted_rf = rf_classifier.predict(X_train)

# Compute training accuracy
train_accuracy_rf = accuracy_score(Y_train, Y_train_predicted_rf)
print("Random Forest Training accuracy:", train_accuracy_rf)

# Make predictions on the test set
Y_test_predicted_rf = rf_classifier.predict(X_test)

# Compute test accuracy
test_accuracy_rf = accuracy_score(Y_test, Y_test_predicted_rf)
print("Random Forest Test accuracy:", test_accuracy_rf)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Assuming X_train, Y_train contain the training features and labels
# Assuming X_test, Y_test contain the test features and labels
# Assuming X_top_features contains the selected features

# Create KNN model
knn_classifier = KNeighborsClassifier(n_neighbors=5)

# Fit on training data
knn_classifier.fit(X_train, Y_train)

# Make predictions on the training set
Y_train_predicted_knn = knn_classifier.predict(X_train)

# Compute training accuracy
train_accuracy_knn = accuracy_score(Y_train, Y_train_predicted_knn)
print("KNN Training accuracy:", train_accuracy_knn)

# Make predictions on the test set
Y_test_predicted_knn = knn_classifier.predict(X_test)

# Compute test accuracy
test_accuracy_knn = accuracy_score(Y_test, Y_test_predicted_knn)
print("KNN Test accuracy:", test_accuracy_knn)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Assuming X_train and Y_train contain the training features and labels
# Assuming X_test and Y_test contain the test features and labels
# Assuming X_top_features contains the selected features

# Create logistic regression model
logreg = LogisticRegression()

# Fit on training data
logreg.fit(X_train, Y_train)

# Make predictions on the training set
Y_train_predicted_lr = logreg.predict(X_train)

# Compute training accuracy
train_accuracy_lr = accuracy_score(Y_train, Y_train_predicted_lr)
print("Logistic Regression Training accuracy:", train_accuracy_lr)

# Make predictions on the test set
Y_test_predicted_lr = logreg.predict(X_test)

# Compute test accuracy
test_accuracy_lr = accuracy_score(Y_test, Y_test_predicted_lr)
print("Logistic Regression Test accuracy:", test_accuracy_lr)

# import numpy as np
# import matplotlib.pyplot as plt
# from sklearn import svm, datasets

# C = 1
# svc = svm.SVC(kernel='linear', C=C).fit(X_train, Y_train)
# lin_svc = svm.LinearSVC(C=C).fit(X_train, Y_train)
# rbf_svc = svm.SVC(kernel='rbf', gamma=1.8, C=C).fit(X_train, Y_train)
# poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X_train, Y_train)


# Y_train_predicted = svc.predict(X_train)
# train_accuracy = accuracy_score(Y_train, Y_train_predicted)
# print(f" svc Train accuracy = ",train_accuracy)


# Y_train_predicted = lin_svc.predict(X_train)
# train_accuracy = accuracy_score(Y_train, Y_train_predicted)
# print(f" svc Train accuracy = ",train_accuracy)



# Y_train_predicted = rbf_svc.predict(X_train)
# train_accuracy = accuracy_score(Y_train, Y_train_predicted)
# print(f" svc Train accuracy = ",train_accuracy)

# X_train = train_data[X_top_features]
# Y_train = train_data['RentCategory']
# logreg = LogisticRegression()

# logreg.fit(X_train, Y_train)

# filename = 'finalized_model.sav'



# loaded_model = pickle.load(open(filename, 'rb'))
# result = loaded_model.score(X_train, Y_train)
# result

# from sklearn.tree import DecisionTreeClassifier
# from sklearn.ensemble import VotingClassifier
# clf1 = LogisticRegression()
# clf2 = DecisionTreeClassifier()
# clf3 = SVC(probability=True)
# voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svm', clf3)], voting='soft')
# voting_clf.fit(X_train, Y_train)

# # Predict on test set

# y_pred_train = voting_clf.predict(X_train)
# y_pred_test = voting_clf.predict(X_test)

# # Calculate accuracy
# train_accuracy = accuracy_score(Y_train, y_pred_train)
# test_accuracy = accuracy_score(Y_test, y_pred_test)
# print("Train Accuracy:", train_accuracy)
#print("Test Accuracy:", test_accuracy)
with open('E:\\Abdelrahman\\Machine_Learning\\Assignments\\Project\\testScript.pkl', 'wb') as f:
    pickle.dump(rf_classifier,f)
with open('E:\\Abdelrahman\\Machine_Learning\\Assignments\\Project\\testScript2.pkl', 'wb') as f:
    pickle.dump(ce, f)
with open('E:\\Abdelrahman\\Machine_Learning\\Assignments\\Project\\testScript3.pkl', 'wb') as f:
    pickle.dump(mam, f)
with open('E:\\Abdelrahman\\Machine_Learning\\Assignments\\Project\\testScript4.pkl', 'wb') as f:
    pickle.dump(city_KP, f)
with open('E:\\Abdelrahman\\Machine_Learning\\Assignments\\Project\\testScript5.pkl', 'wb') as f:
    pickle.dump(state_KP, f)
with open('E:\\Abdelrahman\\Machine_Learning\\Assignments\\Project\\testScript6.pkl', 'wb') as f:
    pickle.dump(photo_ohe, f)
with open('E:\\Abdelrahman\\Machine_Learning\\Assignments\\Project\\testScript7.pkl', 'wb') as f:
    pickle.dump(pets_ohe, f)
with open('E:\\Abdelrahman\\Machine_Learning\\Assignments\\Project\\testScript8.pkl', 'wb') as f:
    pickle.dump(scaler, f)    
with open('E:\\Abdelrahman\\Machine_Learning\\Assignments\\Project\\testScript9.pkl', 'wb') as f:
    pickle.dump(lbls, f)


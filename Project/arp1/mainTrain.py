# -*- coding: utf-8 -*-
"""CS_21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12otGD8ccUROlRLTG9IdSAvJUCbfB0SbT
"""
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import linear_model, metrics, preprocessing, ensemble, model_selection, feature_selection
from scipy.stats import boxcox
from scipy.spatial.distance import cdist
from collections import Counter
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model
from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import r2_score
from Helpers import *

import pickle

"""read the data"""

data = pd.read_csv('ApartmentRentPrediction.csv')

print(data.info())
print(data.shape)

"""put the prediction column at the end"""

predicted_col = data['price_display']
data.drop(columns=['price_display', 'price'], inplace=True)
data['price_display'] = predicted_col

"""modify target format"""
data['price_display'] = data['price_display'].str.replace(r'[^0-9]', '', regex=True)
data['price_display'] = data['price_display'].astype(float)


print(data['price_display'].head(10))

"""number of nulls"""
for column in data.columns:
  print(column, '\t',data[column].isna().sum())

"""replace each row in amenities with the **count** of the amenities"""
ce = CounterEncoder()
data['amenities'] = ce.transform(data['amenities'])
data.head(10)

"""remove noise"""
print(data['category'].value_counts())
data = data.drop(data[(data['category'] == 'housing/rent/short_term') | (data['category'] == 'housing/rent/home')].index)
print(data['category'].value_counts())

mam = MeansAndMods()
mam.fit(data)

data.dropna(subset=['longitude', 'latitude'], inplace=True)

# Note: The following line is Required as we want these 2 cols without any nulls


city_KP = KNN_PP()
data['cityname'] = city_KP.fit_transform(data.loc[:, ['latitude', 'longitude']], data['cityname'])
state_KP = KNN_PP()
data['state'] = state_KP.fit_transform(data.loc[:, ['latitude', 'longitude']], data['state'])

data['bathrooms'] = mam.transform(data['bathrooms'])
data['bedrooms']  = mam.transform(data['bedrooms'])

data['bathrooms'] = data['bathrooms'].astype(int)
data['bedrooms'] = data['bedrooms'].astype(int)


print(data.info())

print(data['pets_allowed'].value_counts())

pets_ohe = OHE(removeCol='pets_allowed')
pets_ohe.fit_transform(data)

print(data['cats_allowed'].value_counts(), data['dogs_allowed'].value_counts())

data['address'].fillna('Unknown', inplace=True)
mam.fit(data)

for col in data.columns:
    data[col] = mam.transform(data[col])

"""number of nulls"""
for column in data.columns:
  print(column, '\t',data[column].isna().sum())

photo_ohe = OHE('has_photo')
photo_ohe.fit_transform(data, 'photo')
data.drop(columns=['no_photo'], inplace=True)

def count_outliers(df):
    for column_name in df.columns:
        Q1 = df[column_name].quantile(0.25)
        Q3 = df[column_name].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]
        print(f"Number of outliers in '{column_name}': {outliers.shape[0]}")

outliercolums = ['bathrooms', 'bedrooms', 'square_feet','price_display']
print(count_outliers(data[outliercolums]))
# pd.set_option('display.max_rows', None)
print(data['bedrooms'].value_counts())

sns.boxplot(data['square_feet'])

sns.boxplot(data['price_display'])

sns.boxplot(data['bedrooms'])

sns.boxplot(data['bathrooms'])

def replace_outliers(df, columns):
    for column_name in columns:
        # Calculate quartiles
        Q1 = df[column_name].quantile(0.25)
        Q3 = df[column_name].quantile(0.75)
        # Calculate IQR
        IQR = Q3 - Q1
        # Define outlier boundaries
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        # Replace outliers below the lower bound with Q1 - 1.5 * IQR
        df.loc[df[column_name] < lower_bound, column_name] = lower_bound
        # Replace outliers above the upper bound with Q3 + 1.5 * IQR
        df.loc[df[column_name] > upper_bound, column_name] = upper_bound
    return df

data = replace_outliers(data, outliercolums)
print(count_outliers(data[outliercolums]))
#print(train_data['square_feet'].max())

data.info()

train_data = data.drop(data[(data['price_type'] == 'Weekly') | (data['price_type'] == 'Monthly|Weekly')].index)
mam.fit(train_data)

cols=('category','title','body','cityname', 'state', 'source', 'address','fee','currency','price_type')

lbls = []
for c in cols:
    lbl = LabelEncoder()
    data[c] = lbl.fit_transform(list(data[c].values))
    lbls.append(lbl)

# from sklearn.preprocessing import MinMaxScaler
# column_names = train_data.columns

# scaler = MinMaxScaler(feature_range=(0, 1))

# train_data_scaled = scaler.fit_transform(train_data)
# train_data_scaled_df = pd.DataFrame(train_data_scaled, columns=column_names)

# train_data = train_data_scaled_df

# test_data_scaled = scaler.transform(test_data)
# test_data_scaled_df = pd.DataFrame(test_data_scaled, columns=column_names)

# test_data = test_data_scaled_df

from sklearn.preprocessing import MinMaxScaler

price_t = data['price_display']
data.drop(columns=['price_display'], inplace = True)

column_names = data.columns

scaler = MinMaxScaler(feature_range=(0, 1))
print(train_data.info())
train_data_scaled = scaler.fit_transform(data)

train_data_scaled_df = pd.DataFrame(train_data_scaled, columns=column_names)

data = train_data_scaled_df
data['price_display'] = price_t.values

data = data.drop('category', axis=1)
data = data.drop('currency', axis=1)
data = data.drop('fee', axis=1)
data = data.drop('price_type', axis=1)

c = data.corr()
top_features = c.index[abs(c['price_display'])>=0.15]
plt.subplots(figsize=(20, 15))
top_corr = data[top_features].corr()
sns.heatmap(top_corr, annot=True)
plt.show()
print(top_features)
top_features = top_features[:-1]
print(top_features)

X = data[top_features]
Y = data['price_display']

# Split data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Check the shapes of the datasets
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("Y_train shape:", Y_train.shape)
print("Y_test shape:", Y_test.shape)

poly_degree = 4 # Changed to degree 4
poly = PolynomialFeatures(degree=poly_degree)

X_train_selected = X_train
X_test_selected = X_test

X_train_poly = poly.fit_transform(X_train_selected)
X_test_poly = poly.transform(X_test_selected)

model = LinearRegression()
model.fit(X_train_poly,Y_train)

# Step 3: Make predictions
train_predictions = model.predict(X_train_poly)
test_predictions = model.predict(X_test_poly)
print(train_predictions)

train_r2 = r2_score(Y_train, train_predictions)
train_mse = metrics.mean_squared_error(Y_train, train_predictions)

test_r2 = r2_score(Y_test, test_predictions)
test_mse = metrics.mean_squared_error(Y_test, test_predictions)

print("Training R^2:", train_r2)
print("Training MSE:", train_mse)
print("Testing R^2:", test_r2)
print("Testing MSE:", test_mse)



from sklearn.linear_model import LinearRegression
from sklearn import metrics

X_train_selected = X_train
X_test_selected = X_test

model = LinearRegression()
model.fit(X_train_selected, Y_train)

y_train_predicted = model.predict(X_train_selected)
y_pred = model.predict(X_test_selected)

train_mse = metrics.mean_squared_error(Y_train, y_train_predicted)
test_mse = metrics.mean_squared_error(Y_test, y_pred)

r2train = r2_score(Y_train, y_train_predicted)
r2test = r2_score(Y_test, y_pred)

print('Training Mean Square Error:', train_mse)
print('Testing Mean Square Error:', test_mse)
print("r2train score:", r2train)
print("r2test score:", r2test)

from sklearn.ensemble import RandomForestRegressor

# Step 2: Train the model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train_selected, Y_train)

# Step 3: Make predictions
rf_y_train_predicted = rf_model.predict(X_train_selected)
rf_y_pred = rf_model.predict(X_test_selected)

# Step 4: Evaluation
rf_train_mse = metrics.mean_squared_error(Y_train, rf_y_train_predicted)
rf_test_mse = metrics.mean_squared_error(Y_test, rf_y_pred)

rf_r2train = r2_score(Y_train, rf_y_train_predicted)
rf_r2test = r2_score(Y_test, rf_y_pred)

print('Random Forest Training Mean Square Error:', rf_train_mse)
print('Random Forest Testing Mean Square Error:', rf_test_mse)
print("Random Forest r2train score:", rf_r2train)
print("Random Forest r2test score:", rf_r2test)

with open('testScript.pkl', 'wb') as f:
    pickle.dump(rf_model,f)
    pickle.dump(ce, f)
    pickle.dump(mam, f)
    pickle.dump(city_KP, f)
    pickle.dump(state_KP, f)
    pickle.dump(photo_ohe, f)
    pickle.dump(pets_ohe, f)
    pickle.dump(scaler, f)
    pickle.dump(lbls, f)

